{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c2125b-e91f-4460-966b-806b6fd8d5b6",
   "metadata": {},
   "source": [
    "## Q1 Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74fbba-08d4-48f9-afde-43c7a7a494eb",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "Overfitting and Underfitting are the two main problems that occur in machine learning and degrade the performance of the machine learning models.\n",
    "\n",
    "Overfitting: Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "The chances of occurrence of overfitting increase as much we provide training to our model. It means the more we train our model, the more chances of occurring the overfitted model.\n",
    "Overfitting is the main problem that occurs in supervised learning.\n",
    "\n",
    "To avoid the Overfitting in Model: Both overfitting and underfitting cause the degraded performance of the machine learning model. But the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.\n",
    "\n",
    "Cross-Validation\n",
    "\n",
    "Training with more data\n",
    "\n",
    "Removing features\n",
    "\n",
    "Early stopping the training\n",
    "\n",
    "Regularization\n",
    "\n",
    "Ensembling\n",
    "\n",
    "Underfitting: Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "An underfitted model has high bias and low variance.\n",
    "\n",
    "To avoid underfitting:\n",
    "    \n",
    "By increasing the training time of the model.\n",
    "\n",
    "By increasing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3202409-9d6f-43fd-9c58-06b53ae6501e",
   "metadata": {},
   "source": [
    "## Q2 How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdc15f-2579-41db-9922-e35e8e05f8be",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "There are several techniques to avoid overfitting in Machine Learning altogether listed below.\n",
    "\n",
    "Cross-Validation\n",
    "\n",
    "Training With More Data\n",
    "\n",
    "Removing Features\n",
    "\n",
    "Early Stopping\n",
    "\n",
    "Regularization\n",
    "\n",
    "Ensembling\n",
    "\n",
    "1. Cross-Validation:\n",
    "\n",
    "One of the most powerful features to avoid/prevent overfitting is cross-validation. The idea behind this is to use the initial training data to generate mini train-test-splits, and then use these splits to tune your model.\n",
    "The cross-validation helps us to tune the hyperparameters with only the original training set. It basically keeps the test set separately as a true unseen data set for selecting the final model. Hence, avoiding overfitting altogether.\n",
    "\n",
    "2. Training With More Data:\n",
    "\n",
    "This technique might not work every time, as we have also discussed in the example above, where training with a significant amount of population helps the model. It basically helps the model in identifying the signal better.\n",
    "But in some cases, the increased data can also mean feeding more noise to the model. When we are training the model with more data, we have to make sure the data is clean and free from randomness and inconsistencies.\n",
    "\n",
    "3. Removing Features:\n",
    "\n",
    "Although some algorithms have an automatic selection of features. For a significant number of those who do not have a built-in feature selection, we can manually remove a few irrelevant features from the input features to improve the generalization.\n",
    "One way to do it is by deriving a conclusion as to how a feature fits into the model. It is quite similar to debugging the code line-by-line.\n",
    "\n",
    "4. Early Stopping:\n",
    "\n",
    "When the model is training, you can actually measure how well the model performs based on each iteration. We can do this until a point when the iterations improve the model’s performance. After this, the model overfits the training data as the generalization weakens after each iteration.\n",
    "So basically, early stopping means stopping the training process before the model passes the point where the model begins to overfit the training data. This technique is mostly used in deep learning.\n",
    "\n",
    "5. Regularization:\n",
    "\n",
    "It basically means, artificially forcing your model to be simpler by using a broader range of techniques. It totally depends on the type of learner that we are using. For example, we can prune a decision tree, use a dropout on a neural network or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "6. Ensembling:\n",
    "\n",
    "This technique basically combines predictions from different Machine Learning models. Two of the most common methods for ensembling are listed below:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting the models\n",
    "\n",
    "Boosting attempts to improve the predictive flexibility of simpler models\n",
    "\n",
    "Even though they are both ensemble methods, the approach totally starts from opposite directions. Bagging uses complex base models and tries to smooth out their predictions while boosting uses simple base models and tries to boost its aggregate complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92329c2e-937a-4ead-9f7e-3be2bc1d9fbf",
   "metadata": {},
   "source": [
    "## Q3 Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486144-8634-4409-978c-25de12abb54d",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "An underfitted model has high bias and low variance.\n",
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset.\n",
    "\n",
    "Reasons for Underfitting:\n",
    "    \n",
    "1) High bias and low variance.\n",
    "\n",
    "2) The size of the training dataset used is not enough.\n",
    "\n",
    "3) The model is too simple.\n",
    "\n",
    "4) Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304af26-ffd0-415d-9705-93c0eb75d6b2",
   "metadata": {},
   "source": [
    "## Q4 Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5b1a9-7285-42a0-979f-bfed70fef6f6",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data. This happens when the hypothesis is too simple or linear in nature.\n",
    "\n",
    "The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data. When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high. While training a data model variance should be kept low.\n",
    "\n",
    "Bias Variance Tradeoff:\n",
    "    \n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cb30e-9db8-4128-b8b2-f1215865cc4d",
   "metadata": {},
   "source": [
    "## Q5 Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129530d-b734-4fea-9719-b1689af2d766",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "How to detect underfitting?\n",
    "\n",
    "A model under fits when it is too simple with regards to the data it is trying to model.\n",
    "One way to detect such a situation is to use the bias-variance approach, which can be represented like this:\n",
    "Your model is under fitted when you have a high bias.\n",
    "\n",
    "How to detect Overfitting?\n",
    "\n",
    "A key challenge with overfitting, and with machine learning in general, is that we can’t know how well our model will perform on new data until we actually test it.\n",
    "To address this, we can split our initial dataset into separate training and test subsets. This method can approximate how well our model will perform on new data.\n",
    "If our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
    "\n",
    "we can determine the difference between an underfitting and overfitting experimentally by comparing fitted models to training-data and test-data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25802a98-e847-4eda-9aa5-48505ca6bb5c",
   "metadata": {},
   "source": [
    "## Q6 Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc9ba0-6b10-4eae-9218-923c10d94161",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "Bias:\n",
    "    \n",
    "Every machine learning algorithm has a prediction error, which can be segmented into three subcomponents: bias error, variance error, and irreducible error. In the process of machine learning, faulty assumptions can lead to the occurrence of a phenomena known as bias.\n",
    "Bias can emerge in the model of machine learning. When an algorithm generates results that are systematically prejudiced due to some inaccurate assumptions that were made throughout the process of machine learning, this is an example of bias.\n",
    "Bias is analogous to a systematic error. They are presumptions that are made by a model in order to simplify the process of learning the target function.\n",
    "A high bias indicates that both the error in the training data and the error in the testing data are greater. To prevent the issue of underfitting, it is usually advised that an algorithm have a minimal bias in order to maximize accuracy.\n",
    "\n",
    "Variance:\n",
    "    \n",
    "The difference in the accuracy of a machine learning model's predictions between the training data and the test data is referred to as variance. A variance error is what we call the situation when a change in the performance of the model is brought about by a variation in the dataset.\n",
    "Variance refers to the magnitude of the change that would occur in the estimation of the target function if a different set of training data was utilized. Because a machine learning algorithm infers the target function from the training data, it is reasonable to anticipate that the method will exhibit some degree of variability.\n",
    "Variance is dependent on a single training set, and it is the factor that determines the inconsistency of the predictions made using various training sets.\n",
    "\n",
    "Examples of high bias and high variance models algorithm include: linear regression, logistic regression and linear discriminant analysis.\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de7c8d-0886-42d9-b69d-3272b68b7ee7",
   "metadata": {},
   "source": [
    "## Q7 What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e962a-5c83-4a60-bab4-66faf8094417",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "Regularization removes extra weights from the selected features and redistributes the weights evenly. This means that regularization discourages the learning of a model of both high complexity and flexibility. A highly flexible model is one that possesses the freedom to fit as many data points as possible.\n",
    "How does Regularization Work?\n",
    "Regularization works by adding a penalty or complexity term to the complex model. Let's consider the simple linear regression equation:\n",
    "\n",
    "y= β0+β1x1+β2x2+β3x3+⋯+βnxn +b\n",
    "In the above equation, Y represents the value to be predicted\n",
    "\n",
    "X1, X2, …Xn are the features for Y.\n",
    "\n",
    "β0,β1,…..βn are the weights or magnitude attached to the features, respectively. Here represents the bias of the model, and b represents the intercept.\n",
    "\n",
    "Linear regression models try to optimize the β0 and b to minimize the cost function. The equation for the cost function for the linear model is given below:\n",
    "\n",
    "RSS=∑i=1n(yi–β0−∑j=1pβjxij)2\n",
    "\n",
    "Now, we will add a loss function and optimize parameter to make the model that can predict the accurate value of Y. The loss function for the linear regression is called as RSS or Residual sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f12a3-3491-4aae-b65b-1f3d90a5447c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
