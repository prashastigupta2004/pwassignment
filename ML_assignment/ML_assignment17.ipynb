{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f005116-e026-4782-82a5-d4bd53f22401",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ad1a3-af71-4cf2-a51a-147d19588d09",
   "metadata": {},
   "source": [
    "Ans=Missing values are usually represented in the form of Nan or null or None in the dataset.\n",
    "Training a model with a data set that has a lot of missing values can drastically impact the quality of machine learning model. Particularly this problem impacts Deterministic models. Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "\n",
    "All the machine learning algorithms donâ€™t support missing values but some ML algorithms are robust to missing values in the dataset. The k-NN algorithm can ignore a column from a distance measure when a value is missing. Naive Bayes can also support missing values when making a prediction. These algorithms can be used when the dataset contains null or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e05c7-07b1-4b90-b67b-b6a5c5aa8dd0",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09560160-126f-405d-a75a-d78bba82e1ab",
   "metadata": {},
   "source": [
    "Ans= \n",
    "\n",
    "Data Dropping:Using the dropna() function is the easiest way to remove observations or features with missing values from the dataframe. Below are some techniques. \n",
    "\n",
    "1) Drop observations with missing values:\n",
    "\n",
    "These three scenarios can happen when trying to remove observations from a data set: \n",
    "\n",
    "dropna(): drops all the rows with missing values.\n",
    "\n",
    "2) Drop columns with missing values\n",
    "\n",
    "The parameter axis = 1 can be used to explicitly specify we are interested in columns rather than rows. \n",
    "\n",
    "dropna(axis = 1): drops all the columns with missing values.\n",
    "\n",
    "### Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sample_customer_data = pd.read_csv(\"data/customer_churn.csv\",  nrows=100)\n",
    "\n",
    "sample_customer_data.info()\n",
    "\n",
    "drop_na_strategy = sample_customer_data.dropna()\n",
    "\n",
    "drop_na_strategy.info()\n",
    "\n",
    "drop_na_cols_strategy = sample_customer_data.dropna(axis=1)\n",
    "\n",
    "drop_na_cols_strategy.info()\n",
    "\n",
    "Mean/Median Imputation:These replacement strategies  are self-explanatory. Mean and median imputations are respectively used to replace  missing values of a given column with the mean and median of the non-missing values in that column. \n",
    "\n",
    "### Example\n",
    "\n",
    "mean_value = sample_customer_data.mean()\n",
    "\n",
    "mean_imputation = sample_customer_data.fillna(mean_value)\n",
    "\n",
    "median_value = sample_customer_data.median()\n",
    "\n",
    "median_imputation = sample_customer_data.fillna(median_value)\n",
    "\n",
    "median_imputation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78d99f-81ff-465d-959d-7f2eec0663e5",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff51ba-c2a6-4e14-b5be-0dcbdde64d0c",
   "metadata": {},
   "source": [
    "Ans=Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations. \n",
    "\n",
    "following happen if imbalanced data is not handled:\n",
    "\n",
    "1)It can discard useful information about the data itself which could be necessary for building rule-based classifiers such as Random Forests.\n",
    "\n",
    "2)The sample chosen by random undersampling may be a biased sample. And it will not be an accurate representation of the population in that case. Therefore, it can cause the classifier to perform poorly on real unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67be72-12d4-457c-aff1-f89abfc7e1ab",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961650ec-bc6e-496e-9522-87c5588fbd27",
   "metadata": {},
   "source": [
    "Ans= To counter imbalanced datasets, we use a technique called up-sampling and down-sampling. Up-sampling and down-sampling work by taking a small subset of the population, and then applying a correction or weighting process to that subset to ensure that the data collected is reflective of the population as a whole. \n",
    "\n",
    "In up-sampling, we randomly duplicate the observations from the minority class in order to reinforce its signal. The most common way is to resample with replacement. This is equivalent to creating a random variable that has mean 0, and variance 1.\n",
    "\n",
    "For example, if we wanted to apply up-sampling on the data in the following table: Class A Class B Class C Number of Observations 5 10 15 Mean 3.5 4 5 Variance 2 2 1Number of Observations resampled with replacement: 5, 10, 15. Mean resampled with replacement: 2.05, 2.75, 3 .5. Variance resampled with replacement: 2, 2, 1. This ensures that the mean and variance of the resampled data are consistent with those of the original data set.\n",
    "\n",
    "In down-sampling, we randomly remove the observations from the majority class. Thus after up-sampling or down-sampling, the dataset becomes balanced with same number of observations in each class .The final result of up-sampling and down-sampling is to preserve the distribution of the data.. .One way of decreasing the variance in a dataset is to down-sample the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c476b89-1134-42a7-9101-df0240999e38",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9028093-255a-47fd-9f00-5d06e03cea9e",
   "metadata": {},
   "source": [
    "Ans= Data augmentation is a set of techniques to artificially increase the amount of data by generating new data points from existing data. This includes making small changes to data or using deep learning models to generate new data points.Data augmentation is useful to improve the performance and outcomes of machine learning models by forming new and different examples to train datasets. \n",
    "\n",
    "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem.\n",
    "It aims to balance class distribution by randomly increasing minority class examples by replicating them.\n",
    "SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ed5f2-f006-4331-ad0a-f48f7ca0eba1",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0288af-e305-46ae-9070-322d066ebc96",
   "metadata": {},
   "source": [
    "Ans= In statistics, any observations or data points that deviate significantly and do not conform with the rest of the observation or data points in a dataset are called outliers. Outliers are extreme values in a feature or dataset.\n",
    "\n",
    "Outliers are also called aberrations, abnormal points, anomalies, etc. It is essential to detect and handle outliers in a dataset as it can have a significant impact on many statistical methods, such as mean, variance, etc., and the performance of the ML models. It can lead to misleading, inconsistent, and inaccurate results if they are not properly accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dca26-35fc-4b35-b7ab-e7ad36783fb4",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebe2aa-e1ee-4bd3-8bbf-a95d322c6465",
   "metadata": {},
   "source": [
    "Ans= \n",
    "\n",
    "1. Mean or Median Imputation: When data is missing at random, we can use list-wise or pair-wise deletion of the missing observations. However, there can be multiple reasons why this may not be the most feasible option:\n",
    "\n",
    "There may not be enough observations with non-missing data to produce a reliable analysis\n",
    "\n",
    "In predictive analytics, missing data can prevent the predictions for those observations which have missing data\n",
    "\n",
    "External factors may require specific observations to be part of the analysis\n",
    "\n",
    "In such cases, we impute values for missing data. A common technique is to use the mean or median of the non-missing observations. This can be useful in cases where the number of missing observations is low. However, for large number of missing values, using mean or median can result in loss of variation in data and it is better to use imputations. Depending upon the nature of the missing data, we use different techniques to impute data that have been described below.\n",
    "\n",
    "2. Use Data Deletion Methods:  The deletion methods only work for certain datasets where entries have missing fields. There are several deleting methods. Three common ones include Listwise Deletion , Pairwise Deletion and Entire Variable Deletion.\n",
    "\n",
    "Listwise Deletion: In this case, an observation (row) containing missing variables are deleted.\n",
    "\n",
    "Pairwise Deletion: We choose to delete cases with MCAR data on the variables we are interested in.  In this case, only the missing observations are ignored. And analysis is done on variables present.\n",
    "\n",
    "Entire Variable Deletion: This case is omitting an entire variable (column) from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5982064-5062-44b1-8b1e-b3dab80624e9",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05dd0d-f628-48d4-88d8-a4e3aa6b93c2",
   "metadata": {},
   "source": [
    "Ans= When data are missing completely at random there is no way to predict where in the data set we'll see a missing value. In an analysis this can often be handled by simply dropping rows of a data set with missing values.\n",
    "\n",
    "When missingness is associated with other variables we call it missing at random. This name is a misnomer. We really mean that conditioned on some of the variables in the data set, the data are missing completely at random. To deal with MAR data we generally predict values for the missing data several times to create multiple data sets that capture the statistical structure of the relationships between the variables and then perform an analysis on the data sets. This procedure is called multiple imputation.\n",
    "\n",
    "The last category, missing not at random is for the case where data is neither MAR nor MCAR. It is usually caused by deterministic relationships between missingness and other measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1e6ce-977c-4bc0-b658-47a881ce85f2",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29cc3e-7951-4d3d-970c-7a14d31c2bd4",
   "metadata": {},
   "source": [
    "Ans= \n",
    "Resampling (Oversampling and Undersampling):\n",
    "    \n",
    "This technique is used to upsample or downsample the minority or majority class. When we are using an imbalanced dataset, we can oversample the minority class using replacement. This technique is called oversampling. Similarly, we can randomly delete rows from the majority class to match them with the minority class which is called undersampling. After sampling the data we can get a balanced dataset for both majority and minority classes. So, when both classes have a similar number of records present in the dataset, we can assume that the classifier will give equal importance to both classes.\n",
    "\n",
    "SMOTE:\n",
    "\n",
    "Synthetic Minority Oversampling Technique or SMOTE is another technique to oversample the minority class. Simply adding duplicate records of minority class often donâ€™t add any new information to the model. In SMOTE new instances are synthesized from the existing data. If we explain it in simple words, SMOTE looks into minority class instances and use k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78976c92-41d5-4aa2-a73a-878a97c32ad9",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541e65a-fe97-44f7-8cfa-d3628292db23",
   "metadata": {},
   "source": [
    "Ans=\n",
    "\n",
    "Under-sampling:Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.\n",
    "\n",
    "Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.The most common heuristic for doing so is resampling without replacement.\n",
    "\n",
    "The process is similar to that of up-sampling. Here are the steps:\n",
    "\n",
    "First, weâ€™ll separate observations from each class into different DataFrames.\n",
    "\n",
    "Next, weâ€™ll resample the majority class without replacement, setting the number of samples to match that of the minority class.\n",
    "\n",
    "Finally, weâ€™ll combine the down-sampled majority class DataFrame with the original minority class DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509e438-452a-407d-b5d1-71c2b77cb23d",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b4747-e8ef-4e33-9b4e-3445945822a3",
   "metadata": {},
   "source": [
    "Ans= Over-sampling: On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique) \n",
    "\n",
    "Up-sample Minority Class: Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.There are several heuristics for doing so, but the most common way is to simply resample with replacement.\n",
    "\n",
    "\n",
    "First, weâ€™ll import the resampling module from Scikit-Learn\n",
    "\n",
    "Next, weâ€™ll create a new DataFrame with an up-sampled minority class. Here are the steps:\n",
    "\n",
    "First, weâ€™ll separate observations from each class into different DataFrames.\n",
    "\n",
    "Next, weâ€™ll resample the minority class with replacement, setting the number of samples to match that of the majority class.\n",
    "\n",
    "Finally, weâ€™ll combine the up-sampled minority class DataFrame with the original majority class DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca4884-e89d-4a9e-b817-3feb2995697f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
