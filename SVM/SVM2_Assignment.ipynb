{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e0a9db-2f65-4ca2-8590-656d0dca14fb",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b7d30-30c6-4790-9b4b-00aca06b9e04",
   "metadata": {},
   "source": [
    "Ans= \n",
    "1) Kernel Functions:\n",
    "\n",
    "1. Kernel functions are used in SVMs to transform data points from the input feature space to a higher-dimensional space. This transformation is often nonlinear and allows SVMs to find nonlinear decision boundaries.\n",
    "2. Kernel functions measure the similarity between two data points in the transformed space, which is crucial for the SVM's decision-making process.\n",
    "3. Common kernel functions include linear kernels, polynomial kernels, radial basis function (RBF) kernels, and sigmoid kernels. Each of these kernel functions defines a different mapping into a higher-dimensional space.\n",
    "\n",
    "2) Polynomial Kernels:\n",
    "\n",
    "1. Polynomial kernels are a specific type of kernel function. They are used when you want to capture polynomial relationships in the data.\n",
    "2. The polynomial kernel of degree d between two data points x and y is defined as (x·y + c)^d, where d is the degree of the polynomial, and c is a constant term.\n",
    "3. When you use a polynomial kernel in an SVM, it effectively computes the dot product of the mapped data points in a higher-dimensional space without explicitly performing the transformation. This allows the SVM to learn polynomial decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0822a-5d62-4b82-b7ec-89fdc0bcd53e",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60276d7b-e1fe-4e32-be21-4b9d67a5df99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "# You can adjust the degree and other hyperparameters as needed\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Fit the SVM classifier to the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = svm_classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd27b1-f71d-42c5-ab4c-f6827a14dedc",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5b486-b454-4b22-bcd9-1559849b32d2",
   "metadata": {},
   "source": [
    "Ans= The relationship between the value of ε and the number of support vectors in SVR is as follows:\n",
    "\n",
    "Smaller Epsilon (ε):\n",
    "\n",
    "1. When you decrease the value of ε, you are making the epsilon-insensitive tube narrower.\n",
    "2. A narrower tube means that data points must stay closer to the predicted function to be within the ε-tube and not be considered errors.\n",
    "3. As ε becomes smaller, the SVR model becomes more sensitive to errors and tries to fit the training data more closely.\n",
    "4. Consequently, reducing ε can lead to an increase in the number of support vectors because the model may need more support vectors to closely approximate the training data points within the narrower tube.\n",
    "\n",
    "Larger Epsilon (ε):\n",
    "\n",
    "1. When you increase the value of ε, you are making the epsilon-insensitive tube wider.\n",
    "2. A wider tube allows data points to be further away from the predicted function without being considered errors.\n",
    "3. As ε becomes larger, the SVR model becomes less sensitive to errors and allows for a larger margin of error.\n",
    "4. Increasing ε tends to reduce the number of support vectors because the model is more tolerant of data points that are farther away from the predicted function.\n",
    "\n",
    "In summary, the choice of ε in SVR affects the trade-off between the model's accuracy and its simplicity (i.e., the number of support vectors). Smaller ε values result in a more complex model with a narrower tube, while larger ε values lead to a simpler model with a wider tube. The specific impact on the number of support vectors depends on the dataset and the relationship between the data points and the function being approximated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768fe66-68c6-4575-8507-3015124bc28e",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ff595-246f-4363-967f-e487e8ade13d",
   "metadata": {},
   "source": [
    "Ans= Choice of Kernel Function:\n",
    "\n",
    "1. Linear Kernel: Suitable for linear relationships between features and the target variable. It tends to produce simpler models with linear decision boundaries.\n",
    "2. Polynomial Kernel: Appropriate when the relationship between features and the target variable is polynomial. You can adjust the polynomial degree using the degree parameter.\n",
    "3. RBF (Radial Basis Function) Kernel: Versatile and effective for capturing complex, nonlinear relationships. The gamma parameter controls the kernel's shape and can have a significant impact on model performance.\n",
    "4. Sigmoid Kernel: Useful when the relationship between features and the target variable is sigmoid-shaped.\n",
    "\n",
    "Example: If you suspect a complex, nonlinear relationship in your data, start with the RBF kernel. If the data is linear, use the linear kernel.\n",
    "\n",
    "C Parameter:\n",
    "\n",
    "1. The C parameter (often referred to as the regularization parameter) controls the trade-off between fitting the training data accurately and minimizing model complexity (avoiding overfitting).\n",
    "2. Smaller C values encourage a larger margin and may result in a simpler model with fewer support vectors but potentially higher training error.\n",
    "3. Larger C values penalize errors more and lead to a narrower margin, potentially capturing more intricate details in the data but risking overfitting.\n",
    "\n",
    "Example: If you have noisy data or believe the training data may contain outliers, consider using a larger C value to minimize the impact of those points. If simplicity and a broader margin are more critical, use a smaller C value.\n",
    "\n",
    "Epsilon (ε) Parameter:\n",
    "\n",
    "1. The epsilon parameter determines the width of the epsilon-insensitive tube around the predicted function.\n",
    "2. Smaller ε values make the tube narrower and the model more sensitive to errors within the tube.\n",
    "3. Larger ε values make the tube wider and the model more tolerant of errors within the tube.\n",
    "\n",
    "Example: If you have a high tolerance for prediction errors or your data is noisy, consider using a larger ε value. If you need precise predictions and are confident in the quality of your data, use a smaller ε value.\n",
    "\n",
    "Gamma (γ) Parameter:\n",
    "\n",
    "1. The gamma parameter influences the shape of the RBF kernel and determines how much each training example affects the decision boundary.\n",
    "2. Smaller gamma values result in a smoother, broader kernel, which can generalize better.\n",
    "3. Larger gamma values lead to a sharper, more localized kernel, which may fit the training data more closely but increase the risk of overfitting.\n",
    "\n",
    "Example: If you have a large dataset, a smaller gamma value is typically preferred as it encourages better generalization. For smaller datasets, you might increase gamma to capture more localized patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817b12c-b5a3-48b8-8d0e-a07d9777d020",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "1) Import the necessary libraries and load the dataset\n",
    "2) Split the dataset into training and testing sets\n",
    "3) Preprocess the data using any technique of your choice (e.g. scaling, normaliMation)\n",
    "4) Create an instance of the SVC classifier and train it on the training data\n",
    "5) Use the trained classifier to predict the labels of the testing data\n",
    "6) Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score)\n",
    "7) Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "8) Train the tuned classifier on the entire dataset\n",
    "8) Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdc226d-2d27-49a8-a2d0-5b0eda0a8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import the necessary libraries and load the dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svc_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')  # You can adjust kernel, C, and gamma as needed\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 7: Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file\n",
    "joblib.dump(best_svc_classifier, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ae8a8-56e4-4a1d-b3d4-a0f901ee3f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
